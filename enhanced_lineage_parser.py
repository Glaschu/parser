#!/usr/bin/env python3
"""
Enhanced SQL Lineage Parser with JSON Metadata Integration
Author: AI Assistant
Purpose: Parse SQL files using sqllineage + merge with C# metadata to produce
         comprehensive end-to-end column lineage mappings
"""

import re
import sqlparse
from sqllineage.runner import LineageRunner
from collections import defaultdict
import argparse
import json
from pathlib import Path

class EnhancedSQLLineageParser:
    """
    Enhanced SQL Lineage Parser that combines sqllineage with JSON metadata
    Focuses on producing clean end-to-end source ‚Üí final target mappings
    """
    
    def __init__(self, sql_file_path, metadata_json_path=None, schema_json_path=None):
        self.sql_file_path = sql_file_path
        self.metadata_json_path = metadata_json_path or "csharp_metadata.json"
        self.schema_json_path = schema_json_path or "schema.json"
        
        self.sql_content = self._read_sql_file()
        self.procedure_body = self._extract_procedure_body()
        
        # Load external data
        self.metadata = self._load_metadata()
        self.schema = self._load_schema()
        
        # Core data structures
        self.source_tables = set()
        self.target_tables = set()
        self.intermediate_tables = set()
        
        # Column flow mapping: source_column -> target_column
        self.column_flows = defaultdict(set)
        self.table_column_map = {}  # table -> [columns]
        self.column_table_map = {}  # column -> table
        
        # Final results
        self.end_to_end_mappings = []
        
    def _read_sql_file(self):
        """Read SQL file content"""
        try:
            with open(self.sql_file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            raise Exception(f"Error reading SQL file: {e}")
    
    def _extract_procedure_body(self):
        """Extract procedure body if it's a stored procedure"""
        proc_match = re.search(r'CREATE\s+PROCEDURE\s+[^\s]+.*?AS\s*BEGIN(.*?)END\s*GO', 
                              self.sql_content, re.DOTALL | re.IGNORECASE)
        if proc_match:
            body = proc_match.group(1).strip()
            print(f"‚úÖ Extracted stored procedure body ({len(body):,} characters)")
            return body
        
        print(f"‚ÑπÔ∏è  Processing as general SQL script ({len(self.sql_content):,} characters)")
        return self.sql_content
    
    def _load_metadata(self):
        """Load C# metadata JSON"""
        try:
            with open(self.metadata_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ Loaded C# metadata from {self.metadata_json_path}")
                return data
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load metadata: {e}")
            return {}
    
    def _load_schema(self):
        """Load schema JSON"""
        try:
            with open(self.schema_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ Loaded schema from {self.schema_json_path}")
                return data
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load schema: {e}")
            return {}
    
    def _build_table_column_mappings(self):
        """Build comprehensive table-column mappings from schema"""
        print("üìä Building table-column mappings from schema...")
        
        for table_name, columns in self.schema.items():
            table_key = table_name.lower()
            self.table_column_map[table_key] = [col.lower() for col in columns.keys()]
            
            # Map each column to its table
            for column_name in columns.keys():
                column_key = f"{table_key}.{column_name.lower()}"
                self.column_table_map[column_key] = table_key
        
        print(f"   ‚úÖ Mapped {len(self.schema)} tables with columns")
    
    def _categorize_tables(self):
        """Categorize tables into source, target, and intermediate"""
        print("üéØ Categorizing tables...")
        
        # Load from metadata if available
        if self.metadata:
            if 'source_tables' in self.metadata:
                real_sources = self.metadata['source_tables'].get('real_tables', [])
                self.source_tables.update(table.lower() for table in real_sources)
            
            if 'target_tables' in self.metadata:
                real_targets = self.metadata['target_tables'].get('real_tables', [])
                self.target_tables.update(table.lower() for table in real_targets)
        
        # Pattern-based categorization for schema tables
        source_patterns = ['staging', 'ref', 'source', 'raw', 'input']
        target_patterns = ['core', 'audit', 'ops', 'final', 'output']
        intermediate_patterns = ['work', 'temp', '#', 'intermediate']
        
        for table in self.table_column_map.keys():
            table_lower = table.lower()
            
            if any(pattern in table_lower for pattern in source_patterns):
                self.source_tables.add(table_lower)
            elif any(pattern in table_lower for pattern in target_patterns):
                self.target_tables.add(table_lower)
            elif any(pattern in table_lower for pattern in intermediate_patterns):
                self.intermediate_tables.add(table_lower)
        
        print(f"   ‚úÖ Categorized {len(self.source_tables)} source tables")
        print(f"   ‚úÖ Categorized {len(self.target_tables)} target tables")
        print(f"   ‚úÖ Categorized {len(self.intermediate_tables)} intermediate tables")
    
    def _extract_sqllineage_flows(self):
        """Extract column flows using sqllineage"""
        print("üîç Extracting column flows with sqllineage...")
        
        # Split SQL into manageable statements
        statements = []
        try:
            statements = sqlparse.split(self.procedure_body)
        except:
            statements = [self.procedure_body]
        
        # Filter for DML statements
        dml_statements = []
        for stmt in statements:
            stmt_clean = stmt.strip()
            if stmt_clean and re.search(r'\b(INSERT|UPDATE|MERGE|SELECT.*INTO|WITH)\b', 
                                       stmt_clean, re.IGNORECASE):
                dml_statements.append(stmt_clean)
        
        print(f"   üìã Processing {len(dml_statements)} DML statements")
        
        processed_count = 0
        for i, stmt in enumerate(dml_statements):
            try:
                # Clean statement
                clean_stmt = re.sub(r'@[a-zA-Z_]\w*', "'placeholder'", stmt)
                clean_stmt = re.sub(r'--.*?$', '', clean_stmt, flags=re.MULTILINE)
                clean_stmt = re.sub(r'/\*.*?\*/', '', clean_stmt, flags=re.DOTALL)
                
                if len(clean_stmt.strip()) < 20:
                    continue
                
                # Parse with sqllineage
                result = LineageRunner(clean_stmt, dialect="tsql")
                column_lineage = result.get_column_lineage()
                
                # Extract flows
                for mapping in column_lineage:
                    if mapping and len(mapping) >= 2:
                        source_col = str(mapping[0]).lower()
                        target_col = str(mapping[-1]).lower()
                        self.column_flows[source_col].add(target_col)
                        
                        # Update column-table mappings
                        if '.' in source_col:
                            source_table = '.'.join(source_col.split('.')[:-1])
                            self.column_table_map[source_col] = source_table
                        if '.' in target_col:
                            target_table = '.'.join(target_col.split('.')[:-1])
                            self.column_table_map[target_col] = target_table
                
                processed_count += 1
                
            except Exception as e:
                # Silently continue on parse errors
                continue
        
        print(f"   ‚úÖ Successfully processed {processed_count} statements")
        print(f"   ‚úÖ Extracted {len(self.column_flows)} column flow mappings")
    
    def _merge_metadata_flows(self):
        """Merge column flows from C# metadata"""
        if not self.metadata or 'column_lineages' not in self.metadata:
            print("‚ö†Ô∏è  No C# metadata column lineages to merge")
            return
        
        print("üîó Merging C# metadata column flows...")
        
        real_to_real = self.metadata['column_lineages'].get('real_to_real', [])
        added_flows = 0
        
        for mapping in real_to_real:
            if not all(key in mapping and mapping[key] for key in 
                      ['source_table', 'source_column', 'target_table', 'target_column']):
                continue
            
            source_table = mapping['source_table'].lower()
            source_column = mapping['source_column'].lower()
            target_table = mapping['target_table'].lower()
            target_column = mapping['target_column'].lower()
            
            source_full = f"{source_table}.{source_column}"
            target_full = f"{target_table}.{target_column}"
            
            self.column_flows[source_full].add(target_full)
            self.column_table_map[source_full] = source_table
            self.column_table_map[target_full] = target_table
            
            added_flows += 1
        
        print(f"   ‚úÖ Added {added_flows} flows from C# metadata")
    
    def _trace_end_to_end_lineage(self):
        """Trace end-to-end lineage from source tables to final target tables"""
        print("üéØ Tracing end-to-end column lineage...")
        
        def find_target_paths(start_col, visited=None, max_depth=5):
            """Find all paths from start column to target table columns"""
            if visited is None:
                visited = set()
            if start_col in visited or max_depth <= 0:
                return []
            
            visited.add(start_col)
            paths = []
            
            # Check if current column is in a target table
            if start_col in self.column_table_map:
                table = self.column_table_map[start_col]
                if table in self.target_tables:
                    return [[start_col]]
            
            # Continue following flows
            for next_col in self.column_flows.get(start_col, []):
                if next_col not in visited:
                    sub_paths = find_target_paths(next_col, visited.copy(), max_depth - 1)
                    for path in sub_paths:
                        paths.append([start_col] + path)
            
            return paths
        
        # Generate end-to-end mappings
        all_mappings = []
        
        # Trace from each source table column
        for source_table in self.source_tables:
            source_columns = self.table_column_map.get(source_table, [])
            
            for source_column in source_columns:
                source_full = f"{source_table}.{source_column}"
                
                # Find all paths to target tables
                paths = find_target_paths(source_full)
                
                for path in paths:
                    if len(path) >= 2:
                        final_column = path[-1]
                        if final_column in self.column_table_map:
                            final_table = self.column_table_map[final_column]
                            
                            source_col_name = source_column
                            final_col_name = final_column.split('.')[-1]
                            
                            all_mappings.append({
                                'source_table': source_table,
                                'source_column': source_col_name,
                                'target_table': final_table,
                                'target_column': final_col_name,
                                'path_length': len(path),
                                'transformation_type': 'traced'
                            })
        
        # Add direct schema-based matches for missing mappings
        schema_mappings = self._generate_schema_based_mappings()
        all_mappings.extend(schema_mappings)
        
        # Remove duplicates and filter
        self.end_to_end_mappings = self._deduplicate_mappings(all_mappings)
        
        print(f"   ‚úÖ Generated {len(self.end_to_end_mappings)} end-to-end mappings")
    
    def _generate_schema_based_mappings(self):
        """Generate mappings based on schema column name matching"""
        print("   üîç Generating schema-based column mappings...")
        
        mappings = []
        
        for source_table in self.source_tables:
            source_columns = self.table_column_map.get(source_table, [])
            
            for target_table in self.target_tables:
                target_columns = self.table_column_map.get(target_table, [])
                
                for source_col in source_columns:
                    for target_col in target_columns:
                        # Direct name match
                        if source_col == target_col:
                            mappings.append({
                                'source_table': source_table,
                                'source_column': source_col,
                                'target_table': target_table,
                                'target_column': target_col,
                                'path_length': 1,
                                'transformation_type': 'schema_exact_match'
                            })
                        # Handle common transformations
                        elif self._is_likely_transformation(source_col, target_col):
                            mappings.append({
                                'source_table': source_table,
                                'source_column': source_col,
                                'target_table': target_table,
                                'target_column': target_col,
                                'path_length': 1,
                                'transformation_type': 'schema_transform_match'
                            })
        
        return mappings
    
    def _is_likely_transformation(self, source_col, target_col):
        """Check if two columns represent a likely transformation"""
        # Common transformation patterns
        transformations = [
            ('txnexternalid', 'idempotencykey'),
            ('accountno', 'accountid'),
            ('txndate', 'postingdate'),
            ('valuedate', 'postingdate'),
            ('batchdate', 'createdat'),
            ('amount', 'amountbase'),
            ('channel', 'feeamount'),  # Channel influences fee calculation
        ]
        
        for src_pattern, tgt_pattern in transformations:
            if src_pattern in source_col and tgt_pattern in target_col:
                return True
        
        # Partial name matches
        if (source_col in target_col or target_col in source_col or
            len(set(source_col.split('_')) & set(target_col.split('_'))) > 0):
            return True
        
        return False
    
    def _deduplicate_mappings(self, mappings):
        """Remove duplicate mappings"""
        seen = set()
        unique_mappings = []
        
        for mapping in mappings:
            key = (mapping['source_table'], mapping['source_column'],
                   mapping['target_table'], mapping['target_column'])
            if key not in seen:
                seen.add(key)
                unique_mappings.append(mapping)
        
        return unique_mappings
    
    def analyze(self):
        """Main analysis method"""
        print("üîç " + "=" * 80)
        print("   ENHANCED SQL LINEAGE ANALYSIS")
        print(f"   File: {self.sql_file_path}")
        print("=" * 82)
        
        # Step 1: Build foundational mappings
        self._build_table_column_mappings()
        self._categorize_tables()
        
        # Step 2: Extract lineage from multiple sources
        self._extract_sqllineage_flows()
        self._merge_metadata_flows()
        
        # Step 3: Generate end-to-end lineage
        self._trace_end_to_end_lineage()
        
        return self.generate_report()
    
    def generate_report(self):
        """Generate the final lineage report"""
        print("\\nüìã " + "=" * 80)
        print("   END-TO-END COLUMN LINEAGE RESULTS")
        print("=" * 82)
        
        if not self.end_to_end_mappings:
            print("‚ùå No end-to-end column mappings found")
            return
        
        # Sort mappings for better presentation
        sorted_mappings = sorted(self.end_to_end_mappings,
                               key=lambda x: (x['target_table'], x['source_table'], x['source_column']))
        
        # Display in the requested format
        print("| Source Column                        | Final Column                      | Final Table        |")
        print("| ------------------------------------ | --------------------------------- | ------------------ |")
        
        for mapping in sorted_mappings:
            source_table = mapping['source_table'].title()
            source_column = mapping['source_column'].title()
            target_table = mapping['target_table'].title()
            target_column = mapping['target_column'].title()
            
            source_full = f"`{source_table}.{source_column}`"
            target_full = f"`{target_table}.{target_column}`"
            target_table_display = f"`{target_table}`"
            
            # Truncate if needed
            if len(source_full) > 38:
                source_full = source_full[:35] + "...`"
            if len(target_full) > 35:
                target_full = target_full[:32] + "...`"
            if len(target_table_display) > 20:
                target_table_display = target_table_display[:17] + "...`"
            
            print(f"| {source_full:<36} | {target_full:<33} | {target_table_display:<18} |")
        
        print(f"\\n‚úÖ Total end-to-end mappings: {len(sorted_mappings)}")
        
        # Summary by transformation type
        by_type = defaultdict(int)
        for mapping in sorted_mappings:
            by_type[mapping['transformation_type']] += 1
        
        print("\\nüìä Mapping breakdown:")
        for trans_type, count in by_type.items():
            print(f"   ‚Ä¢ {trans_type}: {count}")
        
        print("\\n" + "=" * 82)
        print("‚úÖ ENHANCED SQL LINEAGE ANALYSIS COMPLETE!")
        print("=" * 82)
        
        return {
            'end_to_end_mappings': self.end_to_end_mappings,
            'source_tables': list(self.source_tables),
            'target_tables': list(self.target_tables),
            'mapping_count': len(self.end_to_end_mappings)
        }


def main():
    """Command line interface"""
    parser = argparse.ArgumentParser(description='Enhanced SQL Lineage Parser')
    parser.add_argument('sql_file', help='Path to SQL file to analyze')
    parser.add_argument('--metadata', '-m', help='Path to C# metadata JSON', default='csharp_metadata.json')
    parser.add_argument('--schema', '-s', help='Path to schema JSON', default='schema.json')
    parser.add_argument('--export', '-e', help='Export results to JSON file')
    
    args = parser.parse_args()
    
    try:
        # Create and run parser
        parser_instance = EnhancedSQLLineageParser(args.sql_file, args.metadata, args.schema)
        results = parser_instance.analyze()
        
        # Export if requested
        if args.export:
            with open(args.export, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"üìÑ Results exported to {args.export}")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return 1


if __name__ == "__main__":
    import sys
    if len(sys.argv) == 1:
        # Default test run
        parser = EnhancedSQLLineageParser("test.sql", "csharp_metadata.json", "schema.json")
        parser.analyze()
    else:
        sys.exit(main())
